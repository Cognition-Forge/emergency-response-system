# Multi-Provider LLM Support Analysis & Implementation Strategy

## Executive Summary

To support Google Gemini and Claude models alongside OpenAI, **7 core components** require changes. The current OpenAI Agents SDK must be replaced, as it's OpenAI-specific. **Recommendation: Migrate to LangChain** for multi-provider support, rather than LangGraph which would be overkill for this use case.

## Current State Analysis

### AI Integration Architecture
- **Primary Integration**: OpenAI Agents SDK (`openai-agents>=0.3.2`)
- **Client**: `AsyncOpenAI` for API communication
- **Structured Responses**: Handled automatically by Agents SDK
- **Configuration**: Hardcoded OpenAI model names (`gpt-4o-mini`)

### Key Dependencies
```toml
openai>=1.109.1           # OpenAI Python client
openai-agents>=0.3.2      # OpenAI-specific structured response handling
```

### Current Integration Code Structure
The integration is **tightly coupled** to OpenAI through the Agents SDK:
```python
from agents import Agent, Runner, model_settings  # OpenAI Agents SDK
from openai import AsyncOpenAI
```

**Is It Possible to Use Other Providers Currently?**
**Currently: No, not without significant changes.** The system relies on:

1. **OpenAI Agents SDK**: This is OpenAI-specific and handles structured response parsing
2. **OpenAI-specific client**: `AsyncOpenAI` client in `main.py:71`
3. **Model configuration**: Hardcoded to expect OpenAI model names like `"gpt-4o-mini"`

## Components Requiring Changes

### 1. **Core AI Module** (`emergency_accommodation/ai_agent.py`)
- **Current**: Imports `agents.Agent`, `agents.Runner`, `agents.model_settings`
- **Changes Required**: Complete rewrite of AI integration layer
- **Impact**: High - 300+ lines of OpenAI-specific code

### 2. **Main CLI Module** (`emergency_accommodation/main.py`)
- **Current**: `AsyncOpenAI` client creation and management
- **Changes Required**: Abstract client creation for multiple providers
- **Impact**: Medium - Client initialization and configuration

### 3. **Dependencies** (`emergency_accommodation/pyproject.toml`)
- **Current**: OpenAI-specific packages
- **Changes Required**: Add LangChain packages, remove OpenAI Agents SDK
- **Impact**: Low - Dependency declaration

### 4. **Environment Configuration** (`emergency_accommodation/.env.example`)
- **Current**: `OPENAI_API_KEY` only
- **Changes Required**: Support multiple provider API keys
- **Impact**: Low - Add new environment variables

### 5. **Configuration System** (`emergency_accommodation/config/`)
- **Current**: No provider selection mechanism
- **Changes Required**: Add provider selection and model mapping
- **Impact**: Medium - New configuration schemas

### 6. **Test Suite** (`emergency_accommodation/tests/`)
- **Current**: OpenAI-specific mocking and test patterns
- **Changes Required**: Multi-provider test infrastructure
- **Impact**: High - 5+ test files need updates

### 7. **Documentation** (READMEs, setup guides)
- **Current**: OpenAI-focused setup instructions
- **Changes Required**: Multi-provider setup documentation
- **Impact**: Medium - Update setup and configuration docs

## Framework Comparison: OpenAI Agents SDK vs LangChain vs Google ADK vs CrewAI vs LangGraph

### Option 1: Keep OpenAI Agents SDK + Custom Abstraction
**Pros:**
- Minimal changes for OpenAI path
- Proven structured response handling for OpenAI

**Cons:**
- Still need custom integration for Gemini/Claude
- Dual maintenance burden (SDK + custom code)
- No unified interface across providers
- **Verdict: Not Recommended**

### Option 2: Migrate to LangChain
**Pros:**
- Native multi-provider support (OpenAI, Anthropic, Google)
- Unified interface across all providers
- Extensive documentation and community
- Built-in structured output parsing
- Rich ecosystem of tools and integrations
- Enterprise features and Google Cloud integration

**Cons:**
- Learning curve for team
- More dependencies
- Potential over-engineering for simple use case

**LangChain Provider Support:**
```python
# OpenAI
from langchain_openai import ChatOpenAI

# Anthropic (Claude)
from langchain_anthropic import ChatAnthropic

# Google (Gemini) - Two options:
from langchain_google_vertexai import ChatVertexAI        # Enterprise/Cloud
from langchain_google_genai import GoogleGenerativeAI     # Simple/Direct API
```

### Option 3: Custom Multi-Provider with Native SDKs
**Pros:**
- Direct access to latest features
- Minimal overhead and dependencies
- Full control over implementation

**Cons:**
- Significant custom development required
- Need to implement structured output parsing manually
- Maintain separate integration code for each provider
- No unified abstraction layer

**Native SDK Support:**
```python
# OpenAI
from openai import AsyncOpenAI

# Anthropic (Claude)
from anthropic import AsyncAnthropic

# Google (Gemini)
from google import genai
```

### Option 4: Migrate to Google ADK
**Pros:**
- **Google's Official Agent Framework**: Purpose-built for agent development with Google's backing
- **Multi-Provider Support**: Built-in LiteLLM integration for OpenAI, Anthropic, and other providers
- **Structured Output**: Native Pydantic schema support for structured responses
- **Enterprise Ready**: Production-ready with observability, tracing, and deployment features
- **Model Agnostic**: While optimized for Gemini, works with any LLM provider
- **Active Development**: Latest version 0.2.0 (2025), bi-weekly release cadence

**Cons:**
- **Learning Curve**: New framework concepts and patterns to learn
- **Newer Ecosystem**: Less mature than LangChain with fewer third-party integrations
- **Google-Centric**: While model-agnostic, documentation and examples favor Google ecosystem

**Google ADK Multi-Provider Setup:**
```python
from google.adk.agents import LlmAgent
from google.adk.runners import Runner

# Built-in multi-provider support via LiteLLM
agent = LlmAgent(
    name='Emergency Accommodation Evaluator',
    model='openai/gpt-4o-mini',    # or 'anthropic/claude-3-5-sonnet', 'gemini/gemini-pro'
    instruction='Evaluate accommodation options for emergency scenarios',
    output_schema=IterationDecisionPayload  # Pydantic schema support
)
```

### Option 5: Migrate to CrewAI
**Pros:**
- **Specialized Multi-Agent Framework**: Built specifically for role-based agent collaboration
- **Intuitive Design**: Natural modeling of human team structures with roles, goals, and tasks
- **Production-Ready**: High performance optimized for speed and minimal resource usage
- **Enterprise Features**: Built-in observability, tracing, and unified control plane (CrewAI Enterprise Suite)
- **Multi-Provider Support**: Uses LiteLLM for standardized LLM formatting across providers
- **Active Development**: Latest version 0.201.1 (Sep 2025), frequent releases
- **Strong Community**: 100,000+ certified developers, 25k+ GitHub stars

**Cons:**
- **Overkill for Current Use Case**: Designed for complex multi-agent workflows, not single-agent decision making
- **Learning Curve**: New framework concepts around agents, crews, and tasks
- **Architectural Mismatch**: Current system is single-agent focused, not multi-agent collaborative

**CrewAI Multi-Provider Setup:**
```python
from crewai import Agent, Task, Crew
from litellm import completion

# Built-in multi-provider support via LiteLLM
agent = Agent(
    role='Emergency Accommodation Evaluator',
    goal='Evaluate accommodation options for emergency scenarios',
    llm='openai/gpt-4o-mini'    # or 'anthropic/claude-3-5-sonnet', 'gemini/gemini-pro'
)
```

### Option 6: Migrate to LangGraph
**Pros:**
- All LangChain benefits plus advanced workflows
- Stateful agent management
- Complex multi-agent orchestration

**Cons:**
- Overkill for current use case
- Higher complexity than needed
- Steeper learning curve

## Detailed Google Integration Options

### Google ADK vs LangChain Google Integration

**Google ADK (Agent Development Kit) - Released 2025:**
- **Multi-Provider Framework**: Google's official framework for building AI agents, optimized for Gemini but model-agnostic
- **Built-in Multi-Provider Support**: Uses LiteLLM for access to OpenAI, Anthropic, and other providers
- **Structured Output**: Native Pydantic schema support for structured responses
- **Enterprise Ready**: Production-ready with observability and deployment features
- **Multi-Agent Support**: Built for complex agent orchestration while suitable for single agents

**LangChain Google Integration:**
- **langchain-google-vertexai**: Enterprise features, Google Cloud integration, broader model access
- **langchain-google-genai**: Simple prototyping, direct API access through LangChain
- **Unified Interface**: Consistent with other LangChain providers
- **Ecosystem**: Access to LangChain's rich tooling and integrations

## Framework Decision Matrix

| Factor | OpenAI Agents SDK | Google ADK | LangChain | LangGraph | CrewAI |
|--------|-------------------|------------|-----------|-----------|---------|
| **Multi-Provider Support** | ❌ OpenAI only | ✅ Via LiteLLM | ✅ Native | ✅ Native | ✅ Via LiteLLM |
| **Structured Output** | ✅ Built-in | ✅ Pydantic/Schema | ✅ Built-in | ✅ Built-in | ✅ Built-in |
| **Learning Curve** | Low | Medium | Medium | High | Medium |
| **Enterprise Ready** | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes |
| **Community/Support** | ✅ OpenAI backed | ✅ Google backed | ✅ Excellent | ✅ Excellent | ✅ Growing |
| **Development Overhead** | Low | Medium | Medium | High | High |
| **Multi-Agent Support** | ❌ Single agent only | ✅ Built-in orchestration | ✅ Via composition | ✅ Advanced workflows | ✅ Specialized framework |
| **Workflow Management** | ❌ Linear execution | ✅ Agent hierarchies | ✅ Chain composition | ✅ Graph-based flows | ✅ Task-based flows |
| **State Management** | ⚠️ Basic session | ✅ Session management | ✅ Memory systems | ✅ Persistent state | ✅ Crew state |
| **Current Use Case Fit** | ✅ Perfect match | ✅ Suitable | ✅ Perfect match | ❌ Over-engineered | ❌ Over-engineered |

**Verdict: LangChain remains the primary recommendation for current single-agent use case with multi-provider support, with Google ADK as a strong alternative for agent-first development**

## Multi-Agent and Workflow Capabilities Analysis

### Current System Architecture
The emergency accommodation system currently uses a **single-agent architecture** with:
- Linear decision-making process (evaluation → recommendation)
- Session-based state management
- Structured input/output via OpenAI Agents SDK
- No agent collaboration or delegation

### Framework Multi-Agent Capabilities

#### OpenAI Agents SDK
- **Architecture**: Single-agent only
- **Workflow**: Linear execution model
- **State**: Basic session management through SDK
- **Limitations**: Cannot be extended to multi-agent scenarios
- **Use Case**: Perfect for current simple decision workflows

#### Google ADK
- **Architecture**: Built for both single and multi-agent systems
- **Workflow**: Agent hierarchies and delegation patterns
- **State**: Comprehensive session management with agent-to-agent communication
- **Multi-Agent Features**:
  - Agent composition and orchestration
  - Built-in A2A (Agent-to-Agent) protocol support
  - Hierarchical agent structures
  - Tool sharing between agents
- **Use Case**: Excellent for future expansion to multi-agent scenarios

#### LangChain
- **Architecture**: Single-agent with multi-agent composition capabilities
- **Workflow**: Chain-based composition, sequential and parallel execution
- **State**: Memory systems, conversation buffers, and persistent storage
- **Multi-Agent Features**:
  - Agent composition via chains
  - Tool sharing and delegation
  - Memory management across agent interactions
  - Custom orchestration patterns
- **Use Case**: Flexible for both current single-agent and future multi-agent needs

#### LangGraph
- **Architecture**: Graph-based multi-agent orchestration
- **Workflow**: Complex graph-based workflows with conditional branching
- **State**: Persistent state management across complex workflows
- **Multi-Agent Features**:
  - Advanced graph-based agent orchestration
  - Conditional workflows and loops
  - State persistence across complex interactions
  - Visual workflow representation
- **Use Case**: Overkill for current needs, ideal for complex multi-step processes

#### CrewAI
- **Architecture**: Multi-agent collaboration framework
- **Workflow**: Role-based task distribution and collaboration
- **State**: Crew-level state management with agent coordination
- **Multi-Agent Features**:
  - Role-based agent specialization
  - Task delegation and collaboration
  - Built-in agent coordination patterns
  - Hierarchical crew structures
- **Use Case**: Designed specifically for multi-agent scenarios, overkill for single-agent use

### Future Scalability Considerations

**Potential Multi-Agent Scenarios for Emergency Accommodation:**
1. **Specialized Evaluation Agents**: Separate agents for different accommodation types
2. **Regional Coordination**: Multiple agents handling different geographic areas
3. **Stakeholder Communication**: Dedicated agents for different stakeholder types
4. **Resource Validation**: Specialized agents for verifying accommodation availability
5. **Policy Compliance**: Dedicated agents for regulatory and policy checks

**Framework Recommendations by Scenario:**

| Scenario | Best Framework | Reasoning |
|----------|----------------|-----------|
| **Current Single-Agent** | OpenAI Agents SDK / LangChain | Simplicity and proven patterns |
| **Future 2-3 Agents** | Google ADK / LangChain | Good orchestration without complexity |
| **Complex Multi-Agent (5+ agents)** | CrewAI / LangGraph | Specialized multi-agent frameworks |
| **Workflow Automation** | LangGraph | Graph-based workflow management |
| **Google Ecosystem Integration** | Google ADK | Native Google tooling and services |

### Migration Path for Multi-Agent Evolution

**Phase 1 (Current)**: Single-agent decision making
- **Recommended**: LangChain or Google ADK
- **Rationale**: Both support future multi-agent expansion

**Phase 2 (Near Future)**: Specialized agents for different accommodation types
- **Recommended**: Google ADK
- **Rationale**: Built-in agent orchestration and A2A protocol

**Phase 3 (Long Term)**: Complex multi-agent workflows with external integrations
- **Recommended**: CrewAI or LangGraph
- **Rationale**: Advanced multi-agent collaboration patterns

**Verdict: LangChain remains the primary recommendation for current single-agent use case with multi-provider support, with Google ADK as a strong alternative for agent-first development**

## Recommended Implementation Strategy

### Phase 1: Foundation (Week 1-2)
1. **Create Provider Abstraction Layer**

**Option A: LangChain-based Abstraction (Recommended)**
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List

class LLMProvider(ABC):
    @abstractmethod
    async def evaluate_iteration(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        pass

    @abstractmethod
    async def final_recommendation(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        pass

class LangChainProvider(LLMProvider):
    def __init__(self, provider_type: str, model: str, **kwargs):
        if provider_type == "openai":
            from langchain_openai import ChatOpenAI
            self.llm = ChatOpenAI(model=model, **kwargs)
        elif provider_type == "anthropic":
            from langchain_anthropic import ChatAnthropic
            self.llm = ChatAnthropic(model=model, **kwargs)
        elif provider_type == "google":
            from langchain_google_vertexai import ChatVertexAI
            self.llm = ChatVertexAI(model=model, **kwargs)
```

**Option B: Native SDK Abstraction (Alternative)**
```python
# New abstraction layer (if using native SDKs)
class LLMClient(Protocol):
    async def complete(self, messages: List[Dict], model: str, **kwargs) -> Dict

class OpenAIClient(LLMClient): ...
class AnthropicClient(LLMClient): ...
class GoogleClient(LLMClient): ...
```

2. **Update Configuration Schema**
```yaml
# config/providers.yaml
default_provider: "openai"

providers:
  openai:
    models:
      fast: "gpt-4o-mini"
      capable: "gpt-4o"
    api_key_env: "OPENAI_API_KEY"

  anthropic:
    models:
      fast: "claude-3-haiku-20240307"
      capable: "claude-3-5-sonnet-20241022"
    api_key_env: "ANTHROPIC_API_KEY"

  google:
    models:
      fast: "gemini-1.5-flash"
      capable: "gemini-1.5-pro"
    api_key_env: "GOOGLE_API_KEY"
```

**Alternative Configuration System (Native SDKs)**:
```yaml
ai_provider: "openai"  # or "anthropic", "google"
ai_model: "gpt-4o-mini"  # provider-specific model names
provider_config:
  openai:
    api_key: "..."
  anthropic:
    api_key: "..."
  google:
    api_key: "..."
```

3. **Required Code Changes Summary**:
   - **Modify `ai_agent.py`** to work with the abstraction layer
   - **Update model name mapping** for different providers
   - **Abstract the LLM client interface** for consistent API calls

### Phase 2: Core Migration (Week 3-4)
1. **Replace OpenAI Agents SDK with LangChain**
   - Implement structured output parsing using LangChain's built-in tools
   - Migrate prompt management to LangChain prompt templates
   - Update error handling and retry logic

2. **Update Dependencies (Latest Versions as of 2025)**

**Option A: LangChain Approach (Recommended)**
```toml
# Remove
openai>=1.109.1
openai-agents>=0.3.2

# Add - Latest LangChain ecosystem packages
langchain>=0.3.27              # Core LangChain framework
langchain-core>=0.3.76         # Core abstractions and interfaces
langchain-openai>=0.3.33       # OpenAI integration
langchain-anthropic>=0.3.20    # Anthropic/Claude integration

# Google integration options (choose one):
langchain-google-vertexai>=2.1.2   # Enterprise/Cloud features
# OR
langchain-google-genai>=2.0.0      # Simple/Direct API access

langchain-community>=0.3.30    # Community integrations
```

**Option B: Google ADK Approach (Google's Official Agent Framework)**
```toml
# Remove
openai>=1.109.1
openai-agents>=0.3.2

# Add - Google ADK ecosystem
google-adk>=0.2.0             # Google Agent Development Kit (latest 2025)
litellm>=1.0.0                # Multi-provider LLM support (used by ADK)
pydantic>=2.0.0               # For structured output schemas
```

**Option C: Native SDKs Approach (Alternative)**
```toml
# Remove
openai>=1.109.1
openai-agents>=0.3.2

# Add - Native provider SDKs
openai>=1.109.1               # Keep for OpenAI
anthropic>=0.34.0             # Anthropic/Claude native SDK
google-genai>=0.8.0           # Google's unified AI SDK
```

**Option D: CrewAI Approach (Multi-Agent Alternative)**
```toml
# Remove
openai>=1.109.1
openai-agents>=0.3.2

# Add - CrewAI ecosystem
crewai>=0.201.1               # Core CrewAI framework (latest Sep 2025)
crewai-tools>=0.17.0          # Additional tools and integrations
litellm>=1.0.0                # Multi-provider LLM support (used by CrewAI)
```

### Phase 3: Testing & Validation (Week 5-6)
1. **Multi-Provider Test Suite**
   - Mock all three providers consistently
   - Validate structured output parsing across providers
   - Performance benchmarking

2. **Integration Testing**
   - End-to-end tests with real API calls (gated behind env vars)
   - Response quality validation across providers

### Phase 4: Documentation & Deployment (Week 7)
1. **Update Documentation**
   - Multi-provider setup instructions
   - Model selection guidelines
   - Migration guide from current system

## Implementation Details

### Structured Output Parsing with LangChain
```python
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Replaces OpenAI Agents SDK structured responses
parser = PydanticOutputParser(pydantic_object=IterationDecisionPayload)

prompt = ChatPromptTemplate.from_messages([
    ("system", template.combined),
    ("human", "{input}"),
    ("system", "Format your response according to: {format_instructions}")
])

chain = prompt | llm | parser
```

### Environment Configuration
```bash
# .env.example
# Choose your provider
AI_PROVIDER=openai  # or anthropic, google

# Provider API Keys
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key

# Model Selection (optional, defaults from config)
AI_MODEL=gpt-4o-mini  # or claude-3-5-sonnet-20241022, gemini-1.5-pro
```

## Major Implementation Concerns

### 1. Structured Response Parsing
The OpenAI Agents SDK handles complex structured responses automatically. Without it:
- Need to implement custom JSON schema validation
- Handle parsing errors and retries
- Manage prompt engineering for structured outputs

### 2. Provider-Specific Differences
- **Model capabilities**: Not all models support the same structured output quality
- **Token limits**: Different context window sizes
- **API formats**: Different request/response schemas
- **Error handling**: Different error codes and retry strategies

### 3. Development Complexity
- **Testing burden**: Need to mock multiple provider APIs
- **Configuration complexity**: Managing provider-specific settings
- **Maintenance overhead**: Keeping up with multiple provider API changes

### 4. Quality Consistency
- **Prompt optimization**: Each model may require different prompting strategies
- **Response quality**: Different models may produce varying decision quality
- **Scoring consistency**: The scoring logic might behave differently across providers

### Provider-Specific Considerations

#### OpenAI
- **Models**: `gpt-4o-mini`, `gpt-4o`
- **Structured Output**: Native JSON mode support
- **Rate Limits**: Well-established patterns
- **Package**: `langchain-openai>=0.3.33`

#### Anthropic (Claude)
- **Models**: `claude-3-5-sonnet-20241022`, `claude-3-haiku-20240307`
- **Structured Output**: XML-based or JSON with careful prompting
- **Rate Limits**: Different tier structure than OpenAI
- **Package**: `langchain-anthropic>=0.3.20`

#### Google (Gemini)
- **Models**: `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-2.0-flash`
- **Structured Output**: Function calling or JSON mode
- **Authentication**: Service account vs API key options
- **Package Options**:
  - `langchain-google-vertexai>=2.1.2` (LangChain)
  - `google-adk>=0.2.0` with LiteLLM (Google ADK)
  - `google-genai>=0.8.0` (Native SDK)

## Risk Assessment & Mitigation

### High Risk
- **Response Quality Variation**: Different models may produce varying decision quality
  - *Mitigation*: Extensive prompt testing and validation per provider
  - *Validation*: A/B testing with existing scenarios

### Medium Risk
- **API Rate Limits**: Each provider has different limits and error responses
  - *Mitigation*: Implement provider-specific retry logic and rate limiting
  - *Validation*: Load testing with each provider

### Low Risk
- **Configuration Complexity**: Managing multiple providers increases setup complexity
  - *Mitigation*: Clear documentation and sensible defaults
  - *Validation*: User acceptance testing of setup process

## Success Metrics

1. **Functional**: All three providers produce valid structured responses
2. **Performance**: Response times within 20% of current OpenAI implementation
3. **Quality**: Decision quality scores comparable across providers
4. **Reliability**: Error rates below 2% for each provider
5. **Usability**: Setup time under 5 minutes per provider

## Budget Estimate

- **Development Time**: 6-7 weeks
- **Testing Infrastructure**: 1 week
- **Documentation**: 1 week
- **API Costs**: ~$200/month for testing across all providers

## Migration Path Considerations

### Backwards Compatibility
- Maintain existing OpenAI integration during migration
- Feature flag for switching between old and new implementations
- Gradual rollout with fallback mechanisms

### Data Migration
- No database schema changes required
- Configuration files need updates
- Environment variables expansion

## Final Recommendation & Conclusion

### Recommended Approach: LangChain with Google ADK as Strong Alternative

**Primary Recommendation: LangChain** for the following reasons:

1. **Unified Multi-Provider Interface**: Single API across OpenAI, Anthropic, and Google
2. **Proven Structured Output Parsing**: Built-in Pydantic integration eliminates custom parsing
3. **Rich Ecosystem**: Extensive tooling, documentation, and community support
4. **Enterprise Ready**: Mature error handling, retry logic, and observability features
5. **Future-Proof**: Support for emerging providers and model capabilities

**Strong Alternative: Google ADK** for the following reasons:

1. **Google's Official Agent Framework**: Purpose-built for agent development with Google's backing
2. **Native Multi-Provider Support**: Built-in LiteLLM integration for all major providers
3. **Structured Output**: Native Pydantic schema support matches current implementation
4. **Modern Architecture**: Designed specifically for agent workflows and orchestration
5. **Active Development**: Regular updates and alignment with Google's AI roadmap

**Google Integration Decision**:
- **For Enterprise/Cloud**: Use `langchain-google-vertexai` for broader model access and Google Cloud integration
- **For Simplicity**: Use `langchain-google-genai` for direct API access
- **For Agent-First Approach**: Consider Google ADK for advanced agent features and Google ecosystem alignment

**Alternative Considerations**:
- **CrewAI**: While CrewAI (v0.201.1, Sep 2025) is an excellent framework for multi-agent collaboration with built-in multi-provider support via LiteLLM, it's architecturally mismatched for our current single-agent decision-making use case. CrewAI would be ideal for future expansions involving multiple specialized agents working together.

### Implementation Impact Summary

Multi-provider support requires significant architectural changes but is achievable with LangChain. The migration provides long-term benefits including:

- **Vendor Flexibility**: Easy switching between providers based on cost, performance, or capabilities
- **Cost Optimization**: Ability to route different workloads to most cost-effective providers
- **Improved Resilience**: Failover capabilities and reduced vendor lock-in
- **Future Extensibility**: Easy addition of new providers as they emerge

The structured 7-week approach minimizes risk while enabling gradual rollout and validation.

**Next Steps:**
1. Get stakeholder approval for framework choice (LangChain vs Google ADK)
2. Decide between Google integration approaches:
   - `langchain-google-vertexai` for enterprise features
   - `langchain-google-genai` for simplicity
   - Google ADK for agent-first approach
3. Set up development environment with all three provider accounts
4. Begin Phase 1 implementation with provider abstraction layer
5. Update dependencies to chosen framework versions
